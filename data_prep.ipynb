{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "#  Import additional libraries for missing value handling strategies\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from scipy.interpolate import interp1d\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanActivityDataset:\n",
    "    def __init__(self, root_path):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with the root path\n",
    "        \n",
    "        Parameters:\n",
    "        root_path (str): Root path containing 'LS' and 'TS' folders\n",
    "        \"\"\"\n",
    "        self.root_path = root_path\n",
    "        self.learning_set_path = os.path.join(root_path, 'LS')\n",
    "        self.test_set_path = os.path.join(root_path, 'TS')\n",
    "        \n",
    "    def load_dataset(self, dataset_type='learning'):\n",
    "        \"\"\"\n",
    "        Load sensor data, activities, and subject IDs\n",
    "        \n",
    "        Parameters:\n",
    "        dataset_type (str): 'learning' or 'test'\n",
    "        \n",
    "        Returns:\n",
    "        tuple: Sensors dictionary, activities array (None for test set), subjects array\n",
    "        \"\"\"\n",
    "        # Choose the correct path\n",
    "        path = self.learning_set_path if dataset_type == 'learning' else self.test_set_path\n",
    "        \n",
    "        # Dictionary to store sensor data\n",
    "        sensors = {}\n",
    "        \n",
    "        # Load data for each sensor\n",
    "        for i in range(2, 33):  # Sensors from ID 2 to 32\n",
    "            sensor_file = os.path.join(path, f'LS_sensor_{i}.txt' if dataset_type == 'learning' else f'TS_sensor_{i}.txt')\n",
    "            sensors[i] = np.loadtxt(sensor_file)\n",
    "        \n",
    "        # Load subject IDs\n",
    "        subjects = np.loadtxt(os.path.join(path, 'subject_Id.txt'))\n",
    "        \n",
    "        # Load activities for learning set\n",
    "        activities = None\n",
    "        if dataset_type == 'learning':\n",
    "            activities = np.loadtxt(os.path.join(path, 'activity_Id.txt'))\n",
    "        \n",
    "        return sensors, activities, subjects\n",
    "    \n",
    "    def analyze_missing_values(self, sensors):\n",
    "        \"\"\"\n",
    "        Analyze missing values across all sensors\n",
    "        \n",
    "        Parameters:\n",
    "        sensors (dict): Dictionary of sensor data\n",
    "        \n",
    "        Returns:\n",
    "        dict: Missing value statistics for each sensor\n",
    "        \"\"\"\n",
    "        missing_stats = {}\n",
    "        \n",
    "        for sensor_id, sensor_data in sensors.items():\n",
    "            # Count completely missing samples and missing points\n",
    "            missing_samples = np.sum(np.all(sensor_data == -999999.99, axis=1))\n",
    "            missing_points = np.sum(sensor_data == -999999.99)\n",
    "            total_points = sensor_data.size\n",
    "            \n",
    "            missing_stats[sensor_id] = {\n",
    "                'total_samples': sensor_data.shape[0],\n",
    "                'completely_missing_samples': missing_samples,\n",
    "                'missing_points': missing_points,\n",
    "                'missing_percentage': (missing_points / total_points) * 100\n",
    "            }\n",
    "        \n",
    "        return missing_stats\n",
    "    \n",
    "    def visualize_sensor_characteristics(self, sensors, activities):\n",
    "        \"\"\"\n",
    "        Create comprehensive visualizations of sensor data\n",
    "        \n",
    "        Parameters:\n",
    "        sensors (dict): Dictionary of sensor data\n",
    "        activities (array): Activity labels\n",
    "        \"\"\"\n",
    "        # Activity names for reference\n",
    "        activity_names = [\n",
    "            'Lying', 'Sitting', 'Standing', 'Walking very slow', 'Normal walking', \n",
    "            'Nordic walking', 'Running', 'Ascending stairs', 'Descending stairs', \n",
    "            'Cycling', 'Ironing', 'Vacuum cleaning', 'Rope jumping', 'Playing soccer'\n",
    "        ]\n",
    "        \n",
    "        # Select representative sensors from different body locations\n",
    "        selected_sensors = {\n",
    "            'Heart Rate': 2, \n",
    "            'Hand Acceleration': 4, \n",
    "            'Chest Temperature': 13, \n",
    "            'Foot Acceleration': 24, \n",
    "            'Foot Magnetometer': 30\n",
    "        }\n",
    "        \n",
    "        # Create a multi-panel figure\n",
    "        fig, axes = plt.subplots(len(selected_sensors), 1, figsize=(15, 20))\n",
    "        fig.suptitle('Sensor Data Across Different Activities', fontsize=16)\n",
    "        \n",
    "        for i, (sensor_name, sensor_id) in enumerate(selected_sensors.items()):\n",
    "            sensor_data = sensors[sensor_id]\n",
    "            \n",
    "            # Mask missing values\n",
    "            masked_data = np.ma.masked_equal(sensor_data, -999999.99)\n",
    "            \n",
    "            # Box plot of sensor values by activity\n",
    "            box_data = []\n",
    "            for activity in range(1, 15):\n",
    "                activity_data = masked_data[activities == activity]\n",
    "                # Take mean of each sample to reduce dimensionality\n",
    "                box_data.append(np.mean(activity_data, axis=1))\n",
    "            \n",
    "            axes[i].boxplot(box_data, labels=[activity_names[j-1] for j in range(1, 15)])\n",
    "            axes[i].set_title(f'{sensor_name} (Sensor {sensor_id})')\n",
    "            axes[i].set_xlabel('Activities')\n",
    "            axes[i].set_ylabel('Average Sensor Value')\n",
    "            plt.setp(axes[i].get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # def preprocess_sensors(self, sensors):\n",
    "    #     \"\"\"\n",
    "    #     Preprocess sensor data by handling missing values and standardizing\n",
    "        \n",
    "    #     Parameters:\n",
    "    #     sensors (dict): Dictionary of sensor data\n",
    "        \n",
    "    #     Returns:\n",
    "    #     dict: Preprocessed sensor data\n",
    "    #     \"\"\"\n",
    "    #     preprocessed_sensors = {}\n",
    "        \n",
    "    #     # Store scaler for each sensor\n",
    "    #     scalers = {}\n",
    "    #     for sensor_id, sensor_data in sensors.items():\n",
    "    #         # Create a copy of the data\n",
    "    #         processed_data = sensor_data.copy()\n",
    "            \n",
    "    #         # Replace missing values with median for each sample\n",
    "    #         for i in range(processed_data.shape[0]):\n",
    "    #             sample = processed_data[i]\n",
    "    #             valid_values = sample[sample != -999999.99]\n",
    "                \n",
    "    #             if len(valid_values) > 0:\n",
    "    #                 # Replace missing values with sample median\n",
    "    #                 processed_data[i][sample == -999999.99] = np.median(valid_values)\n",
    "    #             else:\n",
    "    #                 # If all values are missing, replace with 0\n",
    "    #                 processed_data[i][sample == -999999.99] = 0\n",
    "            \n",
    "    #         # Standardize each sensor's data\n",
    "    #         scalers[sensor_id] = StandardScaler()\n",
    "    #         preprocessed_sensors[sensor_id] = scalers[sensor_id].fit_transform(processed_data)\n",
    "        \n",
    "    #     return preprocessed_sensors, scalers\n",
    "    \n",
    "    def detect_outliers(self, sensors):\n",
    "        outliers_stats = {}\n",
    "\n",
    "        for sensor_id, data in sensors.items():\n",
    "            Q1 = np.percentile(data, 25)\n",
    "            Q3 = np.percentile(data, 75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "            outliers_percentage = (len(outliers) / data.size) * 100\n",
    "            outliers_stats[sensor_id] = outliers_percentage\n",
    "        return outliers_stats\n",
    "\n",
    "    def explore_dataset(self):\n",
    "        \"\"\"\n",
    "        Comprehensive exploration of the Human Activity Recognition dataset\n",
    "\n",
    "        Returns:\n",
    "        dict: Containing raw and preprocessed sensor data, activities, and subjects\n",
    "        \"\"\"\n",
    "        # Load learning dataset\n",
    "        learning_sensors, activities, learning_subjects = self.load_dataset('learning')\n",
    "\n",
    "        # Load test dataset\n",
    "        test_sensors, _, test_subjects = self.load_dataset('test')\n",
    "\n",
    "        # Analyze missing values for learning set\n",
    "        print(\"Missing Value Statistics (Learning Set):\")\n",
    "        learning_missing_stats = self.analyze_missing_values(learning_sensors)\n",
    "        for sensor_id, stats in learning_missing_stats.items():\n",
    "            print(f\"Sensor {sensor_id}: {stats} missing values\")\n",
    "\n",
    "        # Detect outliers in learning set\n",
    "        print(\"\\nOutlier Statistics (Learning Set):\")\n",
    "        learning_outliers_stats = self.detect_outliers(learning_sensors)\n",
    "        for sensor_id, outliers_percentage in learning_outliers_stats.items():\n",
    "            print(f\"Sensor {sensor_id}: {outliers_percentage:.2f}% outliers detected\")\n",
    "\n",
    "        # Visualize sensor characteristics for learning set\n",
    "        self.visualize_sensor_characteristics(learning_sensors, activities)\n",
    "\n",
    "        # Return key information for further analysis\n",
    "        return {\n",
    "            'learning_raw_sensors': learning_sensors,\n",
    "            'learning_activities': activities,\n",
    "            'learning_subjects': learning_subjects,\n",
    "            'test_raw_sensors': test_sensors,\n",
    "            'test_subjects': test_subjects\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_explorer = HumanActivityDataset('.')\n",
    "\n",
    "# Explore the dataset\n",
    "dataset = dataset_explorer.explore_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sensors_and_activities(root_path='LS'):\n",
    "    # Load activities\n",
    "    activities = np.loadtxt(f'{root_path}/activity_Id.txt')\n",
    "    \n",
    "    # Activity names mapping\n",
    "    activity_names = {\n",
    "        1: 'Lying',\n",
    "        2: 'Sitting', \n",
    "        3: 'Standing',\n",
    "        4: 'Walking very slow',\n",
    "        5: 'Normal walking',\n",
    "        6: 'Nordic walking',\n",
    "        7: 'Running',\n",
    "        8: 'Ascending stairs',\n",
    "        9: 'Descending stairs',\n",
    "        10: 'Cycling',\n",
    "        11: 'Ironing',\n",
    "        12: 'Vacuum cleaning',\n",
    "        13: 'Rope jumping',\n",
    "        14: 'Playing soccer'\n",
    "    }\n",
    "    \n",
    "    # Compute activity distribution\n",
    "    activity_counts = pd.Series(activities).value_counts().sort_index()\n",
    "    \n",
    "    # Create activity distribution plot\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    bars = plt.bar(range(1, 15), activity_counts)\n",
    "    plt.title('Distribution of Activities', fontsize=12)\n",
    "    plt.xlabel('Activity')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(1, 15), [activity_names[i] for i in range(1, 15)], rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print activity distribution percentages\n",
    "    print(\"\\nActivity Distribution Percentages:\")\n",
    "    for act_id, count in activity_counts.items():\n",
    "        percentage = (count/len(activities))*100\n",
    "        print(f\"{activity_names[act_id]}: {percentage:.2f}%\")\n",
    "    \n",
    "    # Compute sensor statistics\n",
    "    sensor_stats = {}\n",
    "    for i in range(2, 33):\n",
    "        sensor_data = np.loadtxt(f'{root_path}/LS_sensor_{i}.txt')\n",
    "        \n",
    "        # Mask missing values\n",
    "        valid_data = sensor_data[sensor_data != -999999.99]\n",
    "        \n",
    "        stats = {\n",
    "            'mean': np.mean(valid_data),\n",
    "            'std': np.std(valid_data),\n",
    "            'min': np.min(valid_data),\n",
    "            'max': np.max(valid_data),\n",
    "            'missing_rate': (np.sum(sensor_data == -999999.99) / sensor_data.size) * 100,\n",
    "            '25%': np.percentile(valid_data, 25),\n",
    "            '50%': np.percentile(valid_data, 50),\n",
    "            '75%': np.percentile(valid_data, 75)\n",
    "        }\n",
    "        sensor_stats[i] = stats\n",
    "    \n",
    "    # Create a DataFrame with sensor statistics\n",
    "    stats_df = pd.DataFrame.from_dict(sensor_stats, orient='index')\n",
    "    \n",
    "    # Print sensor statistics grouped by location\n",
    "    print(\"\\nSensor Statistics:\")\n",
    "    print(\"\\nHeart Rate (Sensor 2):\")\n",
    "    print(stats_df.loc[2].to_string())\n",
    "    \n",
    "    print(\"\\nHand Sensors (3-12):\")\n",
    "    print(stats_df.loc[3:12].to_string())\n",
    "    \n",
    "    print(\"\\nChest Sensors (13-22):\")\n",
    "    print(stats_df.loc[13:22].to_string())\n",
    "    \n",
    "    print(\"\\nAnkle Sensors (23-32):\")\n",
    "    print(stats_df.loc[23:32].to_string())\n",
    "\n",
    "# Run the analysis\n",
    "analyze_sensors_and_activities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive sensor description dictionary\n",
    "sensor_descriptions = {\n",
    "    # Heart Rate\n",
    "    2: {\n",
    "        \"name\": \"Heart Rate\",\n",
    "        \"location\": \"Chest\",\n",
    "        \"type\": \"ECG\",\n",
    "        \"unit\": \"BPM\",\n",
    "        \"missing_rate\": \"0%\",\n",
    "        \"description\": \"Heart rate measurement from chest ECG sensor\"\n",
    "    },\n",
    "    \n",
    "    # Hand IMU (3-12): Accelerometer, Gyroscope, Magnetometer\n",
    "    3: {\"name\": \"Hand Temperature\", \"location\": \"Hand\", \"type\": \"Temperature\", \"unit\": \"°C\", \"missing_rate\": \"4.67%\"},\n",
    "    4: {\"name\": \"Hand Accelerometer X\", \"location\": \"Hand\", \"type\": \"Accelerometer\", \"axis\": \"X\", \"unit\": \"m/s²\", \"missing_rate\": \"4.67%\"},\n",
    "    5: {\"name\": \"Hand Accelerometer Y\", \"location\": \"Hand\", \"type\": \"Accelerometer\", \"axis\": \"Y\", \"unit\": \"m/s²\", \"missing_rate\": \"4.67%\"},\n",
    "    6: {\"name\": \"Hand Accelerometer Z\", \"location\": \"Hand\", \"type\": \"Accelerometer\", \"axis\": \"Z\", \"unit\": \"m/s²\", \"missing_rate\": \"4.67%\"},\n",
    "    7: {\"name\": \"Hand Gyroscope X\", \"location\": \"Hand\", \"type\": \"Gyroscope\", \"axis\": \"X\", \"unit\": \"rad/s\", \"missing_rate\": \"4.67%\"},\n",
    "    8: {\"name\": \"Hand Gyroscope Y\", \"location\": \"Hand\", \"type\": \"Gyroscope\", \"axis\": \"Y\", \"unit\": \"rad/s\", \"missing_rate\": \"4.67%\"},\n",
    "    9: {\"name\": \"Hand Gyroscope Z\", \"location\": \"Hand\", \"type\": \"Gyroscope\", \"axis\": \"Z\", \"unit\": \"rad/s\", \"missing_rate\": \"4.67%\"},\n",
    "    10: {\"name\": \"Hand Magnetometer X\", \"location\": \"Hand\", \"type\": \"Magnetometer\", \"axis\": \"X\", \"unit\": \"μT\", \"missing_rate\": \"4.67%\"},\n",
    "    11: {\"name\": \"Hand Magnetometer Y\", \"location\": \"Hand\", \"type\": \"Magnetometer\", \"axis\": \"Y\", \"unit\": \"μT\", \"missing_rate\": \"4.67%\"},\n",
    "    12: {\"name\": \"Hand Magnetometer Z\", \"location\": \"Hand\", \"type\": \"Magnetometer\", \"axis\": \"Z\", \"unit\": \"μT\", \"missing_rate\": \"4.67%\"},\n",
    "    \n",
    "    # Chest IMU (13-22): Accelerometer, Gyroscope, Magnetometer\n",
    "    13: {\"name\": \"Chest Accelerometer X\", \"location\": \"Chest\", \"type\": \"Accelerometer\", \"axis\": \"X\", \"unit\": \"m/s²\", \"missing_rate\": \"7.10%\"},\n",
    "    14: {\"name\": \"Chest Accelerometer Y\", \"location\": \"Chest\", \"type\": \"Accelerometer\", \"axis\": \"Y\", \"unit\": \"m/s²\", \"missing_rate\": \"7.10%\"},\n",
    "    15: {\"name\": \"Chest Accelerometer Z\", \"location\": \"Chest\", \"type\": \"Accelerometer\", \"axis\": \"Z\", \"unit\": \"m/s²\", \"missing_rate\": \"7.10%\"},\n",
    "    16: {\"name\": \"Chest Gyroscope X\", \"location\": \"Chest\", \"type\": \"Gyroscope\", \"axis\": \"X\", \"unit\": \"rad/s\", \"missing_rate\": \"7.10%\"},\n",
    "    17: {\"name\": \"Chest Gyroscope Y\", \"location\": \"Chest\", \"type\": \"Gyroscope\", \"axis\": \"Y\", \"unit\": \"rad/s\", \"missing_rate\": \"7.10%\"},\n",
    "    18: {\"name\": \"Chest Gyroscope Z\", \"location\": \"Chest\", \"type\": \"Gyroscope\", \"axis\": \"Z\", \"unit\": \"rad/s\", \"missing_rate\": \"7.10%\"},\n",
    "    19: {\"name\": \"Chest Magnetometer X\", \"location\": \"Chest\", \"type\": \"Magnetometer\", \"axis\": \"X\", \"unit\": \"μT\", \"missing_rate\": \"7.10%\"},\n",
    "    20: {\"name\": \"Chest Magnetometer Y\", \"location\": \"Chest\", \"type\": \"Magnetometer\", \"axis\": \"Y\", \"unit\": \"μT\", \"missing_rate\": \"7.10%\"},\n",
    "    21: {\"name\": \"Chest Magnetometer Z\", \"location\": \"Chest\", \"type\": \"Magnetometer\", \"axis\": \"Z\", \"unit\": \"μT\", \"missing_rate\": \"7.10%\"},\n",
    "    22: {\"name\": \"Chest Temperature\", \"location\": \"Chest\", \"type\": \"Temperature\", \"unit\": \"°C\", \"missing_rate\": \"7.10%\"},\n",
    "    \n",
    "    # Ankle IMU (23-32): Accelerometer, Gyroscope, Magnetometer\n",
    "    23: {\"name\": \"Ankle Accelerometer X\", \"location\": \"Ankle\", \"type\": \"Accelerometer\", \"axis\": \"X\", \"unit\": \"m/s²\", \"missing_rate\": \"15.90%\"},\n",
    "    24: {\"name\": \"Ankle Accelerometer Y\", \"location\": \"Ankle\", \"type\": \"Accelerometer\", \"axis\": \"Y\", \"unit\": \"m/s²\", \"missing_rate\": \"15.90%\"},\n",
    "    25: {\"name\": \"Ankle Accelerometer Z\", \"location\": \"Ankle\", \"type\": \"Accelerometer\", \"axis\": \"Z\", \"unit\": \"m/s²\", \"missing_rate\": \"15.90%\"},\n",
    "    26: {\"name\": \"Ankle Gyroscope X\", \"location\": \"Ankle\", \"type\": \"Gyroscope\", \"axis\": \"X\", \"unit\": \"rad/s\", \"missing_rate\": \"15.90%\"},\n",
    "    27: {\"name\": \"Ankle Gyroscope Y\", \"location\": \"Ankle\", \"type\": \"Gyroscope\", \"axis\": \"Y\", \"unit\": \"rad/s\", \"missing_rate\": \"15.90%\"},\n",
    "    28: {\"name\": \"Ankle Gyroscope Z\", \"location\": \"Ankle\", \"type\": \"Gyroscope\", \"axis\": \"Z\", \"unit\": \"rad/s\", \"missing_rate\": \"15.90%\"},\n",
    "    29: {\"name\": \"Ankle Magnetometer X\", \"location\": \"Ankle\", \"type\": \"Magnetometer\", \"axis\": \"X\", \"unit\": \"μT\", \"missing_rate\": \"15.90%\"},\n",
    "    30: {\"name\": \"Ankle Magnetometer Y\", \"location\": \"Ankle\", \"type\": \"Magnetometer\", \"axis\": \"Y\", \"unit\": \"μT\", \"missing_rate\": \"15.90%\"},\n",
    "    31: {\"name\": \"Ankle Magnetometer Z\", \"location\": \"Ankle\", \"type\": \"Magnetometer\", \"axis\": \"Z\", \"unit\": \"μT\", \"missing_rate\": \"15.90%\"},\n",
    "    32: {\"name\": \"Ankle Temperature\", \"location\": \"Ankle\", \"type\": \"Temperature\", \"unit\": \"°C\", \"missing_rate\": \"15.90%\"}\n",
    "}\n",
    "\n",
    "# Print sensor descriptions grouped by location\n",
    "for location in ['Hand', 'Chest', 'Ankle']:\n",
    "    print(f\"\\n=== {location} Sensors ===\")\n",
    "    location_sensors = {k: v for k, v in sensor_descriptions.items() if v['location'] == location}\n",
    "    for sensor_id, desc in sorted(location_sensors.items()):\n",
    "        print(f\"Sensor {sensor_id}: {desc['name']}\")\n",
    "        print(f\"  Type: {desc['type']}\")\n",
    "        print(f\"  Unit: {desc['unit']}\")\n",
    "        print(f\"  Missing Rate: {desc['missing_rate']}\")\n",
    "        if 'axis' in desc:\n",
    "            print(f\"  Axis: {desc['axis']}\")\n",
    "        print()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "total_sensors = len(sensor_descriptions)\n",
    "print(f\"Total number of sensors: {total_sensors}\")\n",
    "print(f\"Number of IMU locations: 3 (Hand, Chest, Ankle)\")\n",
    "print(\"Sensors per IMU:\")\n",
    "print(\"- 1 Temperature sensor\")\n",
    "print(\"- 3 Accelerometer axes (X, Y, Z)\")\n",
    "print(\"- 3 Gyroscope axes (X, Y, Z)\")\n",
    "print(\"- 3 Magnetometer axes (X, Y, Z)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Missing Value Handlers\n",
    "Create a class that inherits from HumanActivityDataset to implement different imputation strategies (mean, mode, knn, interpolation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class that inherits from HumanActivityDataset to implement different imputation strategies\n",
    "class HumanActivityDatasetWithImputation(HumanActivityDataset):\n",
    "    def __init__(self, root_path):\n",
    "        super().__init__(root_path)\n",
    "    \n",
    "    def mean_imputation(self, sensors):\n",
    "        imputed_sensors = {}\n",
    "        scalers = {}\n",
    "        for sensor_id, sensor_data in sensors.items():\n",
    "            imputer = SimpleImputer(strategy='mean')\n",
    "            imputed_data = imputer.fit_transform(sensor_data)\n",
    "            scaler = StandardScaler()\n",
    "            scaled_data = scaler.fit_transform(imputed_data)\n",
    "            imputed_sensors[sensor_id] = scaled_data\n",
    "            scalers[sensor_id] = scaler\n",
    "        return imputed_sensors, scalers\n",
    "    \n",
    "    def mode_imputation(self, sensors):\n",
    "        imputed_sensors = {}\n",
    "        scalers = {}\n",
    "        for sensor_id, sensor_data in sensors.items():\n",
    "            imputer = SimpleImputer(strategy='most_frequent')\n",
    "            imputed_data = imputer.fit_transform(sensor_data)\n",
    "            scaler = StandardScaler()\n",
    "            scaled_data = scaler.fit_transform(imputed_data)\n",
    "            imputed_sensors[sensor_id] = scaled_data\n",
    "            scalers[sensor_id] = scaler\n",
    "        return imputed_sensors, scalers\n",
    "    \n",
    "    def knn_imputation(self, sensors, n_neighbors=5):\n",
    "        imputed_sensors = {}\n",
    "        scalers = {}\n",
    "        for sensor_id, sensor_data in sensors.items():\n",
    "            imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "            imputed_data = imputer.fit_transform(sensor_data)\n",
    "            scaler = StandardScaler()\n",
    "            scaled_data = scaler.fit_transform(imputed_data)\n",
    "            imputed_sensors[sensor_id] = scaled_data\n",
    "            scalers[sensor_id] = scaler\n",
    "        return imputed_sensors, scalers\n",
    "    \n",
    "    def linear_interpolation(self, sensors):\n",
    "        imputed_sensors = {}\n",
    "        scalers = {}\n",
    "        for sensor_id, sensor_data in sensors.items():\n",
    "            imputed_data = sensor_data.copy()\n",
    "            for i in range(imputed_data.shape[1]):\n",
    "                col = imputed_data[:, i]\n",
    "                mask = col != -999999.99\n",
    "                if np.any(mask):\n",
    "                    interp_func = interp1d(np.where(mask)[0], col[mask], bounds_error=False, fill_value=\"extrapolate\")\n",
    "                    col[~mask] = interp_func(np.where(~mask)[0])\n",
    "                imputed_data[:, i] = col\n",
    "            scaler = StandardScaler()\n",
    "            scaled_data = scaler.fit_transform(imputed_data)\n",
    "            imputed_sensors[sensor_id] = scaled_data\n",
    "            scalers[sensor_id] = scaler\n",
    "        return imputed_sensors, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset explorer with imputation strategies\n",
    "dataset_explorer_imputation = HumanActivityDatasetWithImputation('.')\n",
    "\n",
    "# Load the learning dataset\n",
    "learning_sensors, activities, learning_subjects = dataset_explorer_imputation.load_dataset('learning')\n",
    "\n",
    "# Apply different imputation strategies\n",
    "mean_imputed_sensors, mean_scalers = dataset_explorer_imputation.mean_imputation(learning_sensors)\n",
    "mode_imputed_sensors, mode_scalers = dataset_explorer_imputation.mode_imputation(learning_sensors)\n",
    "knn_imputed_sensors, knn_scalers = dataset_explorer_imputation.knn_imputation(learning_sensors)\n",
    "linear_imputed_sensors, linear_scalers = dataset_explorer_imputation.linear_interpolation(learning_sensors)\n",
    "\n",
    "# Save the imputed datasets and scalers\n",
    "joblib.dump(mean_imputed_sensors, os.path.join(dataset_explorer_imputation.learning_set_path, 'mean_imputed_sensors.pkl'))\n",
    "joblib.dump(mean_scalers, os.path.join(dataset_explorer_imputation.learning_set_path, 'mean_scalers.pkl'))\n",
    "joblib.dump(mode_imputed_sensors, os.path.join(dataset_explorer_imputation.learning_set_path, 'mode_imputed_sensors.pkl'))\n",
    "joblib.dump(mode_scalers, os.path.join(dataset_explorer_imputation.learning_set_path, 'mode_scalers.pkl'))\n",
    "joblib.dump(knn_imputed_sensors, os.path.join(dataset_explorer_imputation.learning_set_path, 'knn_imputed_sensors.pkl'))\n",
    "joblib.dump(knn_scalers, os.path.join(dataset_explorer_imputation.learning_set_path, 'knn_scalers.pkl'))\n",
    "joblib.dump(linear_imputed_sensors, os.path.join(dataset_explorer_imputation.learning_set_path, 'linear_imputed_sensors.pkl'))\n",
    "joblib.dump(linear_scalers, os.path.join(dataset_explorer_imputation.learning_set_path, 'linear_scalers.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why We Impute with Different Methods\n",
    "\n",
    "Imputation is the process of replacing missing data with substituted values. Different imputation methods are used to handle missing data in various ways:\n",
    "\n",
    "- **Mean Imputation**: Replaces missing values with the mean of the available values. This method is simple and effective when the data is symmetrically distributed.\n",
    "- **Mode Imputation**: Replaces missing values with the most frequent value (mode). This is useful for categorical data or when the data has a high frequency of certain values.\n",
    "- **KNN Imputation**: Uses the k-nearest neighbors algorithm to impute missing values based on the values of the nearest neighbors. This method can capture the local structure of the data.\n",
    "- **Linear Interpolation**: Estimates missing values by fitting a linear function to the available data points. This method is useful when the data follows a linear trend.\n",
    "\n",
    "Using different imputation methods allows us to compare their effectiveness and choose the best approach for our specific dataset.\n",
    "\n",
    "### Why We Use .pkl Files and How to Work with Them\n",
    "\n",
    "`.pkl` files are used to serialize Python objects using the `pickle` module. Serialization is the process of converting an object into a byte stream, which can be saved to a file and later deserialized to reconstruct the original object. We use `.pkl` files for the following reasons:\n",
    "\n",
    "- **Efficiency**: Pickle files are efficient for saving and loading complex data structures, such as dictionaries and lists.\n",
    "- **Convenience**: They allow us to save the state of an object, such as a trained model or preprocessed data, and load it later without having to recompute or retrain.\n",
    "- **Portability**: Pickle files can be easily shared and used across different environments.\n",
    "\n",
    "To work with `.pkl` files, we use the `joblib` library, which provides efficient serialization for large numpy arrays and other data structures:\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Save an object to a .pkl file\n",
    "joblib.dump(object, 'filename.pkl')\n",
    "\n",
    "# Load an object from a .pkl file\n",
    "object = joblib.load('filename.pkl')\n",
    "```\n",
    "\n",
    "### Why We Standardize the Values and Why We Are Saving the Scalers for Later\n",
    "\n",
    "Standardization is the process of scaling data to have a mean of 0 and a standard deviation of 1. This is important for the following reasons:\n",
    "\n",
    "- **Consistency**: Standardizing ensures that all features contribute equally to the analysis, preventing features with larger scales from dominating. We indeed have very different metrics (BPM vs C°)\n",
    "- **Improved Performance**: Many machine learning algorithms perform better when the data is standardized, as it helps with convergence and stability during training.\n",
    "- **Comparability**: Standardized data allows for easier comparison between different features and datasets.\n",
    "\n",
    "We save the scalers for later use to ensure that the same transformation is applied to new data, such as test data or future data points. This maintains consistency and allows us to accurately compare and evaluate the performance of our models:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Fit and transform the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Load the scaler and transform new data\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "new_scaled_data = scaler.transform(new_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Imputation Methods\n",
    "Show statistics to compare how different imputation methods handle the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Imputation Methods\n",
    "\n",
    "# Create visualizations and statistics to compare how different imputation methods handle the missing values\n",
    "\n",
    "# Function to visualize imputed data\n",
    "def visualize_imputed_data(imputed_sensors, title):\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(15, 25))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    selected_sensors = [2, 4, 13, 24, 30]  # Example sensor IDs\n",
    "    for i, sensor_id in enumerate(selected_sensors):\n",
    "        sensor_data = imputed_sensors[sensor_id]\n",
    "        axes[i].plot(sensor_data)\n",
    "        axes[i].set_title(f'Sensor {sensor_id}')\n",
    "        axes[i].set_xlabel('Sample')\n",
    "        axes[i].set_ylabel('Value')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# # Visualize mean imputed data\n",
    "# visualize_imputed_data(mean_imputed_sensors, 'Mean Imputation')\n",
    "\n",
    "# # Visualize mode imputed data\n",
    "# visualize_imputed_data(mode_imputed_sensors, 'Mode Imputation')\n",
    "\n",
    "# # Visualize k-NN imputed data\n",
    "# visualize_imputed_data(knn_imputed_sensors, 'k-NN Imputation')\n",
    "\n",
    "# # Visualize linear interpolation imputed data\n",
    "# visualize_imputed_data(linear_imputed_sensors, 'Linear Interpolation')\n",
    "\n",
    "# Function to calculate and print statistics for imputed data\n",
    "def print_imputation_statistics(imputed_sensors, method_name):\n",
    "    print(f\"Statistics for {method_name}:\")\n",
    "    for sensor_id, sensor_data in imputed_sensors.items():\n",
    "        mean_value = np.mean(sensor_data)\n",
    "        std_dev = np.std(sensor_data)\n",
    "        print(f\"Sensor {sensor_id}: Mean = {mean_value}, Std Dev = {std_dev}\")\n",
    "\n",
    "# Print statistics for mean imputed data\n",
    "print_imputation_statistics(mean_imputed_sensors, 'Mean Imputation')\n",
    "\n",
    "# Print statistics for mode imputed data\n",
    "print_imputation_statistics(mode_imputed_sensors, 'Mode Imputation')\n",
    "\n",
    "# Print statistics for k-NN imputed data\n",
    "print_imputation_statistics(knn_imputed_sensors, 'k-NN Imputation')\n",
    "\n",
    "# Print statistics for linear interpolation imputed data\n",
    "print_imputation_statistics(linear_imputed_sensors, 'Linear Interpolation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sensor_outliers(root_path='LS'):\n",
    "    # Define sensor groups and their valid ranges\n",
    "    sensor_groups = {\n",
    "        'heart_rate': {'sensors': [2], 'range': (30, 220), 'unit': 'bpm'},\n",
    "        'temperature': {'sensors': [3, 13, 23], 'range': (20, 40), 'unit': '°C'},\n",
    "        'acceleration': {'sensors': [4,5,6, 14,15,16, 24,25,26], 'range': (-50, 50), 'unit': 'm/s²'},\n",
    "        'gyroscope': {'sensors': [7,8,9, 17,18,19, 27,28,29], 'range': (-20, 20), 'unit': 'rad/s'},\n",
    "        'magnetometer': {'sensors': [10,11,12, 20,21,22, 30,31,32], 'range': (-100, 100), 'unit': 'µT'}\n",
    "    }\n",
    "    \n",
    "    outlier_stats = defaultdict(dict)\n",
    "    \n",
    "    # Create subplots for each sensor type\n",
    "    fig, axes = plt.subplots(len(sensor_groups), 1, figsize=(15, 5*len(sensor_groups)))\n",
    "    fig.suptitle('Sensor Values Distribution with Outliers Highlighted', fontsize=16)\n",
    "    \n",
    "    for idx, (sensor_type, info) in enumerate(sensor_groups.items()):\n",
    "        all_values = []\n",
    "        outlier_counts = []\n",
    "        \n",
    "        for sensor_id in info['sensors']:\n",
    "            # Load sensor data\n",
    "            sensor_data = np.loadtxt(f'{root_path}/LS_sensor_{sensor_id}.txt')\n",
    "            valid_data = sensor_data[sensor_data != -999999.99]\n",
    "            \n",
    "            # Calculate outliers based on valid range\n",
    "            outliers_range = valid_data[(valid_data < info['range'][0]) | (valid_data > info['range'][1])]\n",
    "            \n",
    "            # Calculate statistical outliers (IQR method)\n",
    "            Q1 = np.percentile(valid_data, 25)\n",
    "            Q3 = np.percentile(valid_data, 75)\n",
    "            IQR = Q3 - Q1\n",
    "            statistical_outliers = valid_data[(valid_data < Q1 - 1.5*IQR) | (valid_data > Q3 + 1.5*IQR)]\n",
    "            \n",
    "            # Store statistics\n",
    "            outlier_stats[sensor_type][sensor_id] = {\n",
    "                'total_samples': len(sensor_data),\n",
    "                'valid_samples': len(valid_data),\n",
    "                'range_outliers': len(outliers_range),\n",
    "                'statistical_outliers': len(statistical_outliers),\n",
    "                'range_outlier_pct': (len(outliers_range) / len(valid_data)) * 100,\n",
    "                'statistical_outlier_pct': (len(statistical_outliers) / len(valid_data)) * 100\n",
    "            }\n",
    "            \n",
    "            all_values.extend(valid_data)\n",
    "            outlier_counts.append(len(outliers_range))\n",
    "        \n",
    "        # Plot distribution\n",
    "        sns.histplot(all_values, bins=100, ax=axes[idx])\n",
    "        axes[idx].axvline(info['range'][0], color='r', linestyle='--', alpha=0.5)\n",
    "        axes[idx].axvline(info['range'][1], color='r', linestyle='--', alpha=0.5)\n",
    "        axes[idx].set_title(f'{sensor_type.capitalize()} Sensors ({info[\"unit\"]})')\n",
    "        axes[idx].set_xlabel(f'Value ({info[\"unit\"]})')\n",
    "        axes[idx].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nOutlier Analysis Summary:\")\n",
    "    for sensor_type, sensors in outlier_stats.items():\n",
    "        print(f\"\\n{sensor_type.upper()} SENSORS:\")\n",
    "        for sensor_id, stats in sensors.items():\n",
    "            print(f\"\\nSensor {sensor_id}:\")\n",
    "            print(f\"  Valid samples: {stats['valid_samples']:,}\")\n",
    "            print(f\"  Range outliers: {stats['range_outliers']:,} ({stats['range_outlier_pct']:.2f}%)\")\n",
    "            print(f\"  Statistical outliers: {stats['statistical_outliers']:,} ({stats['statistical_outlier_pct']:.2f}%)\")\n",
    "\n",
    "# Run the analysis\n",
    "analyze_sensor_outliers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions\n",
    "\n",
    "We have several sensors that could be combined as they represent values on a 3D scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
